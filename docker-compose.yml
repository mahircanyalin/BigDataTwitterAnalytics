services:
  # ==========================================
  # HDFS (Hadoop 3.2.1) - Depolama Katmanı
  # ==========================================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CLUSTER_NAME=termproject
      # İzin Hatalarını Çözen Proxy User Ayarları (ÇOK ÖNEMLİ)
      - CORE_CONF_hadoop_proxyuser_root_hosts=*
      - CORE_CONF_hadoop_proxyuser_root_groups=*
      - CORE_CONF_hadoop_proxyuser_hive_hosts=*
      - CORE_CONF_hadoop_proxyuser_hive_groups=*
      - CORE_CONF_hadoop_proxyuser_spark_hosts=*
      - CORE_CONF_hadoop_proxyuser_spark_groups=*
    ports:
      - "9870:9870" # Web UI
      - "9000:9000" # IPC
    volumes:
      - namenode:/hadoop/dfs/name
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9870" ]
      interval: 10s
      timeout: 5s
      retries: 5

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CLUSTER_NAME=termproject
      - SERVICE_PRECONDITION=namenode:9870
      # İzin Ayarları
      - CORE_CONF_hadoop_proxyuser_root_hosts=*
      - CORE_CONF_hadoop_proxyuser_root_groups=*
      - CORE_CONF_hadoop_proxyuser_hive_hosts=*
      - CORE_CONF_hadoop_proxyuser_hive_groups=*
    depends_on:
      namenode:
        condition: service_healthy
    volumes:
      - datanode:/hadoop/dfs/data
    ports:
      - "9864:9864"

  # ==========================================
  # Hive (2.3.2) & Metastore - Veri Kataloğu
  # ==========================================
  hive-metastore-db:
    image: postgres:9.6
    container_name: hive-metastore-db
    restart: always
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hivepassword
    volumes:
      - hive-db:/var/lib/postgresql/data

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    restart: always
    # Metastore servisinin doğru başlaması için bu komut şart:
    command: /opt/hive/bin/hive --service metastore
    depends_on:
      - namenode
      - datanode
      - hive-metastore-db
    ports:
      - "9083:9083"
    environment:
      - SERVICE_PRECONDITION=namenode:9870 datanode:9864 hive-metastore-db:5432
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore-db:5432/metastore
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName=org.postgresql.Driver
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName=hive
      - HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword=hivepassword
      - HIVE_SITE_CONF_datanucleus_autoCreateSchema=false
      - HIVE_SITE_CONF_hive_metastore_uris=thrift://hive-metastore:9083
      - HIVE_METASTORE_SCHEMA_VERIFICATION=false

      # İzin Ayarları
      - CORE_CONF_hadoop_proxyuser_root_hosts=*
      - CORE_CONF_hadoop_proxyuser_root_groups=*
      - CORE_CONF_hadoop_proxyuser_hive_hosts=*
      - CORE_CONF_hadoop_proxyuser_hive_groups=*

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    restart: always
    depends_on:
      - hive-metastore
    ports:
      - "10000:10000"
    command: /opt/hive/bin/hiveserver2
    environment:
      - SERVICE_PRECONDITION=hive-metastore:9083
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HIVE_SITE_CONF_hive_metastore_uris=thrift://hive-metastore:9083
      # İzin Ayarları (Beeline bağlantısı için kritik)
      - CORE_CONF_hadoop_proxyuser_root_hosts=*
      - CORE_CONF_hadoop_proxyuser_root_groups=*
      - CORE_CONF_hadoop_proxyuser_hive_hosts=*
      - CORE_CONF_hadoop_proxyuser_hive_groups=*

  # ==========================================
  # Spark (Batch Processing)
  # ==========================================
  spark:
    image: apache/spark:3.5.0
    container_name: spark
    ports:
      - "4040:4040"
      - "8080:8080"
    depends_on:
      namenode:
        condition: service_healthy
      hive-metastore:
        condition: service_started
    environment:
      # Spark'ın Hive Metastore'u bulması için:
      - SPARK_HIVE_METASTORE_URIS=thrift://hive-metastore:9083
    # Konteyneri ayakta tutmak için:
    command: /bin/bash -c "/opt/spark/sbin/start-master.sh && sleep infinity"

  # ==========================================
  # Kafka & Zookeeper (Streaming Source)
  # ==========================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.3
    hostname: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.5.3
    ports:
      - "29092:29092"
      - "9092:9092"

    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  # ==========================================
  # Flink (Streaming Processing)
  # ==========================================
  flink-jobmanager:
    image: flink:1.18.1-java11
    container_name: flink-jobmanager
    command: jobmanager
    ports:
      - "8081:8081"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    volumes:
      - ./flink_jobs:/opt/flink/usrlib
      - ./flink_plugins:/opt/flink/plugins

  flink-taskmanager:
    image: flink:1.18.1-java11
    container_name: flink-taskmanager
    command: taskmanager
    depends_on:
      - flink-jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - TASK_MANAGER_NUMBER_OF_TASK_SLOTS=2
    volumes:
      - ./flink_jobs:/opt/flink/usrlib
      - ./flink_plugins:/opt/flink/plugins

volumes:
  namenode:
  datanode:
  hive-db:
